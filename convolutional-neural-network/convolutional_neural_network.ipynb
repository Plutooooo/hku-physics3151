{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network ( CNN )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>In deep learning, a convolutional neural network (CNN) is a class of deep neural networks, most commonly applied to analyzing visual imagery. We introduce it here with two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>The first example is using CNN to identify images of digits.\n",
    "<br>Data source:https://www.kaggle.com/dingli/digits-recognition-with-cnn-keras/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>The images are kept as matrices, each row containing 784 pixels and label of the digit in the first column as shown below. Considering the limit of memory here, we just take part of the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://quantummc.xyz/wp-content/uploads/2020/07/hku_phys3151_cnn_example1.csv\")\n",
    "train = train[0:4200]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>From the training data, we take the labels as Y (result) and the rest as X (input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>As shown above, we count the numbers of different digits, and the result turns out that they have close values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>To have a view at the relationship between the vectors of pixels and the images, we take the 4th row of the training data as example, we show the image and use the label as the title, which match well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_eg=np.array(X_train)[3].reshape(28,28)\n",
    "plt.imshow(img_eg,cmap='gray')\n",
    "plt.title(np.array(Y_train)[3])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Next, we set the value of pixels to be within the range of 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "#test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Since we want the pixels of each image to be matrices and Keras (which we are gonna use to build the CNN) needs an extra dimension in the end which correspond to channels(it's 1 here because the images are gray scaled), we reshape the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(4200,28,28,1)\n",
    "#test = test.values.reshape(28000,28,28,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>The initial Y are the labels which are 0,1,2.... For identification, we convert them into vectors, and thus for example:<br> 2 becomes [0,0,1,0,0,0,0,0,0,0] <br> 0 becomes [1,0,0,0,0,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>At the last step of data preparation, split the data into train and test sets(here we take only $\\frac{1}{10}$ of the data as test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>There are different parts in CNN:\n",
    "<br>1.$\\underline{Convolution\\ \\ Layer}$: Take a small matrix as the feature(kernel, filter), which can be a n by n matrix. Successively, convolve each part(size determined by feature) of the initial matrix and the feature, and we get a feature map as a result.\n",
    "<br>To avoid decreasing the size after convolution, we use $\\underline{padding}$ here, normally adding 0s to the edges.\n",
    "<br><br>2.$\\underline{Activation\\ \\ Function}$: we use Relu(Rectified Linear Units) here, whose fomula is simple: $f(x)=max(0,x)$, and thus the negative values are turned into 0 while the positive values remain.\n",
    "<br><br>3.$\\underline{Pooling}$: Pooling can shrink a large image while retaining important information. Here, we use Max Pooling: Usually, the pooling matrix is 2 by 2 size, the maximum value in the 2\\*2 size block in the input image is taken as the resulting pixel value.\n",
    "<br><br>4.$\\underline{Fully\\ \\ Connected\\ \\ Layers}$: In the fully connected layers below, we flatten the pooled feature maps into vectors. Every value we get in the vectors votes for the result, realized by the activation functions. Here, we also include softmax function, the formula of which is: $P=\\frac{e^{x^{T}W_j}}{\\sum_ke^{x^TW_k}}$. The result represents the probability of the type of the output.\n",
    "<br><br>In the CNN built below, Dropout is used, where randomly selected neurons are ignored during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (28,28,1))) \n",
    "#kernel size refers to the size of the feature matrix\n",
    "#filters refer to the number of output filters in the convolution\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))      #strides refer to the factor by which to downscale.\n",
    "model.add(Dropout(0.25))\n",
    "#fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))   #the first number in the () of Dense refers to the dimension of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>After the construction of the strcture of CNN, we need to set the optimizer. Here we choose Adam, which is an algorithm for performing a stepwise optimization on stochastic objective functions, and the parameters are the ones mostly used in machine learning.\n",
    "<br>For the loss, we use categorical_crossentropy:$$CCE = -\\frac{1}{N}\\sum_{i=0}^N\\sum_{j=0}^Jy_{i,j}\\cdot log(P_{i,j})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>The training process can consist of several epochs, one epoch means one forward pass and one backward pass of all the training examples and batch size means the number of training examples in one forward/backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10  # for better result increase the epochs\n",
    "batch_size=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Then, we fit the model with both the training set and validation set which evaluates the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train, batch_size = None,epochs = epochs, validation_data = (X_val,Y_val), validation_steps=X_train.shape[0] // batch_size, steps_per_epoch=X_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>After we trained the model, plot the accuracy and loss versus epochs of both the train and validation(test) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "ax[0,0].plot(history.history['acc'])\n",
    "ax[0,0].set_xlabel('Epoch')\n",
    "ax[0,0].set_ylabel('Accuracy')\n",
    "ax[0,0].set_title('Accuracy of Train')\n",
    "ax[0,1].plot(history.history['loss'])\n",
    "ax[0,1].set_xlabel('Epoch')\n",
    "ax[0,1].set_ylabel('Loss')\n",
    "ax[0,1].set_title('Loss of Train')\n",
    "ax[1,0].plot(history.history['val_acc'])\n",
    "ax[1,0].set_xlabel('Epoch')\n",
    "ax[1,0].set_ylabel('Accuracy')\n",
    "ax[1,0].set_title('Accuracy of Validation')\n",
    "ax[1,1].plot(history.history['val_loss'])\n",
    "ax[1,1].set_xlabel('Epoch')\n",
    "ax[1,1].set_ylabel('Loss')\n",
    "ax[1,1].set_title('Loss of Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>The accuracy is acceptable to some extent.\n",
    "<br>For more direct view of the fit, plot the confusion matrix of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) #return the max of the axis\n",
    "Y_true = np.argmax(Y_val,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,linecolor=\"gray\", fmt= '.1f',ax=ax)#fmt refers to type of the data\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Through the confusion matrix, we can see exactly how well the test set are predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Inspired by the paper Super-resolving the Ising model with convolutional neural networks(https://arxiv.org/abs/1810.02372), we try to use CNN here to restore the decimated configurations of the Ising model back to the initial size with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>We got initial data from Monte Carlo, and they are 200 configurations of 16 by 16 matrices for each T of totally 61 different Ts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "csvFile = open(\"cnn_example2.csv\")\n",
    "reader = csv.reader(csvFile)\n",
    "rows= [row for row in reader]\n",
    "cdata=np.array(rows)\n",
    "data_allT=np.zeros((61,np.array(rows[0]).size))\n",
    "for i in range(0,61):\n",
    "    data_allT[i]=cdata[2*i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_separate=data_allT.reshape(61,int(data_allT[0].size/(16*16)),16*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Our purpose is to get the initial configurations from the decimated ones, so we set the initial 16*16 matrices as y, and since we'll use sigmoid functions later, we turn the spin value of -1 into 0 while the spin value of 1 remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Y_train=copy.deepcopy(data_separate.reshape(-1,256))\n",
    "for i in range(0,len(Y_train)):\n",
    "    for j in range(0,256):\n",
    "        if Y_train[i,j]==-1:\n",
    "            Y_train[i,j]=0\n",
    "        else:\n",
    "            Y_train[i,j]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>To do the decimation, the rule is that each 16*16 matrix can be divided into 8*8 blocks, and each block then be replaced by a single value. If the sum of a block >=0, then the value is 1, or else it's -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_dec=np.zeros((int(data_allT.size/(16*16)),64))\n",
    "data_T=copy.deepcopy(data_separate.reshape(-1,256))\n",
    "for i in range(0,int(data_allT.size/(16*16))):\n",
    "    ini=data_T[i]\n",
    "    for ii in range(0,8):\n",
    "        for iii in range(0,8):\n",
    "            x=2*ii+32*iii\n",
    "            sum=ini[x]+ini[x+1]+ini[x+16]+ini[x+17]\n",
    "            if sum >= 0:\n",
    "                data_dec[i,ii+iii*8]=1\n",
    "            else:\n",
    "                data_dec[i,ii+iii*8]=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>To fit the model, add an additional dimension to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train=data_dec.reshape(-1,8,8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>As we did before, separate the data into train set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Unlike the periodic boundary used in the paper to avoid size decreasing, we just use the normal padding and choose linear activation function this time in the convolution layer. In the final layer, use sigmoid function, since the values we want in the configuration are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D,Activation\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(filters = 6, kernel_size = (5,5),padding = 'Same',activation ='linear'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(filters = 12, kernel_size = (3,3),padding = 'Same',activation ='linear'))\n",
    "#fully connected\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(256, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>As we used sigmoid function at last, choose binary_crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model2.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10  # for better result increase the epochs\n",
    "batch_size = 600\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>With model constructed and parameters set, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model2.fit(X_train,Y_train, batch_size = None,epochs = epochs, validation_data = (X_val,Y_val), validation_steps=X_train.shape[0] // batch_size, steps_per_epoch=X_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('The final accuracy of the train set is :',history.history['acc'][-1])\n",
    "print('The final accuracy of the validation set is :',history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>From the accuracy, we can tell that the fit is accaptable for 16 by 16 matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>First, get the predictions of the decimated matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pre=model2.predict(data_dec.reshape(-1,8,8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Remember that we previously turned the the spin into 0 and 1, so we need to change them back into -1 and 1 by choosing the values >=0.5 as 1 and others as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pre_re=copy.deepcopy(pre)\n",
    "for i in range(0,len(pre_re)):\n",
    "    for j in range(0,256):\n",
    "        if pre_re[i,j]>=0.5:\n",
    "            pre_re[i,j]=1\n",
    "        else:\n",
    "            pre_re[i,j]=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow>Then we try to get the average magnetization of the 200 samples of each T, and plot the predicted results and initial ones together to evaluate our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "M_real=[]\n",
    "M_predict=[]\n",
    "for i in range(0,61):\n",
    "    M1=0\n",
    "    M2=0\n",
    "    forT1=data_separate.reshape(-1,256)[i*200:i*200+200]\n",
    "    forT2=pre_re[i*200:i*200+200]\n",
    "    for j in range(0,200):\n",
    "        M1=M1+abs(np.sum(forT1[j])/256)\n",
    "        M2=M2+abs(np.sum(forT2[j])/256)\n",
    "    M_real.append(M1/200)\n",
    "    M_predict.append(M2/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Temp=np.arange(1,4.05,0.05)\n",
    "plt.plot(Temp,M_real,label='exact',ls='--',marker='o')\n",
    "plt.plot(Temp,M_predict,label='predict',ls='--',marker='.')\n",
    "plt.xlabel(r\"$T$\")\n",
    "plt.ylabel(r\"$Magnetization$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td bgcolor=lightyellow> From the figure, the model is well fitted. The initial sample base are small and so are not that satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
