{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network (FNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron (No hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
      "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "5          2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "6          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "7          3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "8          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "9          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "10         3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "11         5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "12         3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "13         6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "14         1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "15         7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "16         2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "17         8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "18         6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "19         9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "20         4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "21         0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "22         9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "23         1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "24         1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "25         2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "26         4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "27         3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "28         2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "29         7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
      "59970      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59971      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59972      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59973      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59974      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59975      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59976      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59977      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59978      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59979      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59980      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59981      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59982      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59983      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59984      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59985      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59986      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59987      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59988      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59989      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59990      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59991      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59992      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59993      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59994      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "\n",
      "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
      "0          0      0      0      0      0      0      0      0  \n",
      "1          0      0      0      0      0      0      0      0  \n",
      "2          0      0      0      0      0      0      0      0  \n",
      "3          0      0      0      0      0      0      0      0  \n",
      "4          0      0      0      0      0      0      0      0  \n",
      "5          0      0      0      0      0      0      0      0  \n",
      "6          0      0      0      0      0      0      0      0  \n",
      "7          0      0      0      0      0      0      0      0  \n",
      "8          0      0      0      0      0      0      0      0  \n",
      "9          0      0      0      0      0      0      0      0  \n",
      "10         0      0      0      0      0      0      0      0  \n",
      "11         0      0      0      0      0      0      0      0  \n",
      "12         0      0      0      0      0      0      0      0  \n",
      "13         0      0      0      0      0      0      0      0  \n",
      "14         0      0      0      0      0      0      0      0  \n",
      "15         0      0      0      0      0      0      0      0  \n",
      "16         0      0      0      0      0      0      0      0  \n",
      "17         0      0      0      0      0      0      0      0  \n",
      "18         0      0      0      0      0      0      0      0  \n",
      "19         0      0      0      0      0      0      0      0  \n",
      "20         0      0      0      0      0      0      0      0  \n",
      "21         0      0      0      0      0      0      0      0  \n",
      "22         0      0      0      0      0      0      0      0  \n",
      "23         0      0      0      0      0      0      0      0  \n",
      "24         0      0      0      0      0      0      0      0  \n",
      "25         0      0      0      0      0      0      0      0  \n",
      "26         0      0      0      0      0      0      0      0  \n",
      "27         0      0      0      0      0      0      0      0  \n",
      "28         0      0      0      0      0      0      0      0  \n",
      "29         0      0      0      0      0      0      0      0  \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "59970      0      0      0      0      0      0      0      0  \n",
      "59971      0      0      0      0      0      0      0      0  \n",
      "59972      0      0      0      0      0      0      0      0  \n",
      "59973      0      0      0      0      0      0      0      0  \n",
      "59974      0      0      0      0      0      0      0      0  \n",
      "59975      0      0      0      0      0      0      0      0  \n",
      "59976      0      0      0      0      0      0      0      0  \n",
      "59977      0      0      0      0      0      0      0      0  \n",
      "59978      0      0      0      0      0      0      0      0  \n",
      "59979      0      0      0      0      0      0      0      0  \n",
      "59980      0      0      0      0      0      0      0      0  \n",
      "59981      0      0      0      0      0      0      0      0  \n",
      "59982      0      0      0      0      0      0      0      0  \n",
      "59983      0      0      0      0      0      0      0      0  \n",
      "59984      0      0      0      0      0      0      0      0  \n",
      "59985      0      0      0      0      0      0      0      0  \n",
      "59986      0      0      0      0      0      0      0      0  \n",
      "59987      0      0      0      0      0      0      0      0  \n",
      "59988      0      0      0      0      0      0      0      0  \n",
      "59989      0      0      0      0      0      0      0      0  \n",
      "59990      0      0      0      0      0      0      0      0  \n",
      "59991      0      0      0      0      0      0      0      0  \n",
      "59992      0      0      0      0      0      0      0      0  \n",
      "59993      0      0      0      0      0      0      0      0  \n",
      "59994      0      0      0      0      0      0      0      0  \n",
      "59995      0      0      0      0      0      0      0      0  \n",
      "59996      0      0      0      0      0      0      0      0  \n",
      "59997      0      0      0      0      0      0      0      0  \n",
      "59998      0      0      0      0      0      0      0      0  \n",
      "59999      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[60000 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from pandas import DataFrame\n",
    "\n",
    "#df = pd.read_csv(\"asset/csv/mnist_train.csv\", sep=\",\")\n",
    "df = pd.read_csv(\"https://quantummc.xyz/wp-content/uploads/2020/08/feedforward-neural-network-example-1.csv\", sep=\",\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'label')\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron in this case is simply a feed-forward neural network with no hidden layers. This is equivalent to a multivariate logistic regression, or a Softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def computecost(theta,X,y,alpha):\n",
    "    m = X.shape[0] #First we get the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(X,theta) #Then we compute raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    cost = (-1 / m) * np.sum(y_mat * np.log(prob)) + (alpha/2)*np.sum(theta*theta) #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(X.T,(y_mat - prob)) + alpha*theta #And compute the gradient for that loss\n",
    "    return cost,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getProbsAndPreds(someX):\n",
    "    probs = softmax(np.dot(someX,theta))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros([X.shape[1],len(np.unique(y))])\n",
    "iterations = 100\n",
    "learningRate = 1e-5\n",
    "losses = []\n",
    "\n",
    "def gradient_descent(X,y,theta,alpha=0.01,iterations=100):\n",
    "    cost_history = np.zeros(iterations)\n",
    "    for it in range(iterations):\n",
    "        cost,grad = computecost(theta,X,y,alpha)\n",
    "        theta = theta - (learningRate * grad)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "theta,cost_history = gradient_descent(X,y,theta,learningRate,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"theta.csv\", theta, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from pandas import DataFrame\n",
    "\n",
    "#df = pd.read_csv(\"asset/csv/mnist_test.csv\", sep=\",\")\n",
    "df = pd.read_csv(\"https://quantummc.xyz/wp-content/uploads/2020/08/feedforward-neural-network-example-2.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'label')\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prob,pred=getProbsAndPreds(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('The accuracy of this model is:', 100*accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hidden Layer (Multiple Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the torch package to make our single hidden layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the dataset iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 60000\n",
    "n_iters = 100\n",
    "#num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "#num_epochs = int(num_epochs)\n",
    "num_epochs=100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define what kind of neural network we want to set up. Here it is a feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use a Rectified Linear Unit activation because it provides faster convergence than sigmoid or tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our images are 28x28 pixels, so our input dimensions is 784 dimensions. We will have a hidden layer of 100 hidden neurons. And our output will be 10, because there are 0-9 digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 200\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetModel(\n",
       "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cross entropy loss because this is the most appropriate loss function for logistic/softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SGD optimizer defined within the torch package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.3364226818084717. Accuracy: 11.64\n",
      "Iteration: 2. Loss: 2.298659324645996. Accuracy: 12.07\n",
      "Iteration: 3. Loss: 2.290571689605713. Accuracy: 13.06\n",
      "Iteration: 4. Loss: 2.284876823425293. Accuracy: 15.13\n",
      "Iteration: 5. Loss: 2.279186725616455. Accuracy: 17.64\n",
      "Iteration: 6. Loss: 2.2734782695770264. Accuracy: 20.73\n",
      "Iteration: 7. Loss: 2.267773389816284. Accuracy: 23.39\n",
      "Iteration: 8. Loss: 2.262007236480713. Accuracy: 26.37\n",
      "Iteration: 9. Loss: 2.2562143802642822. Accuracy: 29.34\n",
      "Iteration: 10. Loss: 2.250336170196533. Accuracy: 31.97\n",
      "Iteration: 11. Loss: 2.2444510459899902. Accuracy: 34.3\n",
      "Iteration: 12. Loss: 2.238450050354004. Accuracy: 36.38\n",
      "Iteration: 13. Loss: 2.232363224029541. Accuracy: 38.44\n",
      "Iteration: 14. Loss: 2.226207733154297. Accuracy: 40.54\n",
      "Iteration: 15. Loss: 2.2199504375457764. Accuracy: 42.16\n",
      "Iteration: 16. Loss: 2.2135767936706543. Accuracy: 43.58\n",
      "Iteration: 17. Loss: 2.2070932388305664. Accuracy: 44.8\n",
      "Iteration: 18. Loss: 2.200483560562134. Accuracy: 46.15\n",
      "Iteration: 19. Loss: 2.1937336921691895. Accuracy: 47.28\n",
      "Iteration: 20. Loss: 2.1868367195129395. Accuracy: 48.43\n",
      "Iteration: 21. Loss: 2.1798064708709717. Accuracy: 49.63\n",
      "Iteration: 22. Loss: 2.1726090908050537. Accuracy: 50.48\n",
      "Iteration: 23. Loss: 2.1652286052703857. Accuracy: 51.33\n",
      "Iteration: 24. Loss: 2.1577224731445312. Accuracy: 52.31\n",
      "Iteration: 25. Loss: 2.1500065326690674. Accuracy: 53.29\n",
      "Iteration: 26. Loss: 2.142090320587158. Accuracy: 54.14\n",
      "Iteration: 27. Loss: 2.134014844894409. Accuracy: 54.93\n",
      "Iteration: 28. Loss: 2.1257171630859375. Accuracy: 55.68\n",
      "Iteration: 29. Loss: 2.1172378063201904. Accuracy: 56.33\n",
      "Iteration: 30. Loss: 2.108548164367676. Accuracy: 56.98\n",
      "Iteration: 31. Loss: 2.0996429920196533. Accuracy: 57.6\n",
      "Iteration: 32. Loss: 2.0905158519744873. Accuracy: 58.15\n",
      "Iteration: 33. Loss: 2.0811734199523926. Accuracy: 58.77\n",
      "Iteration: 34. Loss: 2.071606397628784. Accuracy: 59.25\n",
      "Iteration: 35. Loss: 2.061819076538086. Accuracy: 59.68\n",
      "Iteration: 36. Loss: 2.051799774169922. Accuracy: 60.2\n",
      "Iteration: 37. Loss: 2.041555643081665. Accuracy: 60.62\n",
      "Iteration: 38. Loss: 2.031078815460205. Accuracy: 61.06\n",
      "Iteration: 39. Loss: 2.02038311958313. Accuracy: 61.53\n",
      "Iteration: 40. Loss: 2.0094492435455322. Accuracy: 61.93\n",
      "Iteration: 41. Loss: 1.9982935190200806. Accuracy: 62.38\n",
      "Iteration: 42. Loss: 1.986922264099121. Accuracy: 62.74\n",
      "Iteration: 43. Loss: 1.975293755531311. Accuracy: 63.15\n",
      "Iteration: 44. Loss: 1.963477611541748. Accuracy: 63.55\n",
      "Iteration: 45. Loss: 1.9514282941818237. Accuracy: 63.98\n",
      "Iteration: 46. Loss: 1.9392023086547852. Accuracy: 64.32\n",
      "Iteration: 47. Loss: 1.9267445802688599. Accuracy: 64.76\n",
      "Iteration: 48. Loss: 1.9140936136245728. Accuracy: 65.23\n",
      "Iteration: 49. Loss: 1.9012459516525269. Accuracy: 65.5\n",
      "Iteration: 50. Loss: 1.8882296085357666. Accuracy: 65.77\n",
      "Iteration: 51. Loss: 1.8750064373016357. Accuracy: 66.12\n",
      "Iteration: 52. Loss: 1.861649513244629. Accuracy: 66.47\n",
      "Iteration: 53. Loss: 1.8481091260910034. Accuracy: 66.78\n",
      "Iteration: 54. Loss: 1.834432601928711. Accuracy: 67.17\n",
      "Iteration: 55. Loss: 1.8206048011779785. Accuracy: 67.36\n",
      "Iteration: 56. Loss: 1.806662678718567. Accuracy: 67.58\n",
      "Iteration: 57. Loss: 1.7926090955734253. Accuracy: 67.94\n",
      "Iteration: 58. Loss: 1.778423547744751. Accuracy: 68.24\n",
      "Iteration: 59. Loss: 1.764153003692627. Accuracy: 68.56\n",
      "Iteration: 60. Loss: 1.74980628490448. Accuracy: 68.91\n",
      "Iteration: 61. Loss: 1.7353734970092773. Accuracy: 69.12\n",
      "Iteration: 62. Loss: 1.7208924293518066. Accuracy: 69.42\n",
      "Iteration: 63. Loss: 1.706369400024414. Accuracy: 69.75\n",
      "Iteration: 64. Loss: 1.6917883157730103. Accuracy: 70.02\n",
      "Iteration: 65. Loss: 1.677183985710144. Accuracy: 70.19\n",
      "Iteration: 66. Loss: 1.6625746488571167. Accuracy: 70.38\n",
      "Iteration: 67. Loss: 1.6479657888412476. Accuracy: 70.64\n",
      "Iteration: 68. Loss: 1.6333743333816528. Accuracy: 70.9\n",
      "Iteration: 69. Loss: 1.6187920570373535. Accuracy: 71.23\n",
      "Iteration: 70. Loss: 1.604218602180481. Accuracy: 71.56\n",
      "Iteration: 71. Loss: 1.5897127389907837. Accuracy: 71.89\n",
      "Iteration: 72. Loss: 1.5752501487731934. Accuracy: 72.19\n",
      "Iteration: 73. Loss: 1.5608556270599365. Accuracy: 72.38\n",
      "Iteration: 74. Loss: 1.5465248823165894. Accuracy: 72.65\n",
      "Iteration: 75. Loss: 1.5322643518447876. Accuracy: 72.86\n",
      "Iteration: 76. Loss: 1.5181033611297607. Accuracy: 73.11\n",
      "Iteration: 77. Loss: 1.5040401220321655. Accuracy: 73.25\n",
      "Iteration: 78. Loss: 1.4900789260864258. Accuracy: 73.5\n",
      "Iteration: 79. Loss: 1.4762084484100342. Accuracy: 73.76\n",
      "Iteration: 80. Loss: 1.462478756904602. Accuracy: 73.97\n",
      "Iteration: 81. Loss: 1.4488580226898193. Accuracy: 74.11\n",
      "Iteration: 82. Loss: 1.4353654384613037. Accuracy: 74.33\n",
      "Iteration: 83. Loss: 1.422019124031067. Accuracy: 74.62\n",
      "Iteration: 84. Loss: 1.40880286693573. Accuracy: 74.83\n",
      "Iteration: 85. Loss: 1.395731806755066. Accuracy: 74.96\n",
      "Iteration: 86. Loss: 1.3828057050704956. Accuracy: 75.09\n",
      "Iteration: 87. Loss: 1.3700330257415771. Accuracy: 75.34\n",
      "Iteration: 88. Loss: 1.3574012517929077. Accuracy: 75.57\n",
      "Iteration: 89. Loss: 1.3449395895004272. Accuracy: 75.76\n",
      "Iteration: 90. Loss: 1.33263099193573. Accuracy: 75.91\n",
      "Iteration: 91. Loss: 1.3204777240753174. Accuracy: 76.13\n",
      "Iteration: 92. Loss: 1.3084803819656372. Accuracy: 76.43\n",
      "Iteration: 93. Loss: 1.296657919883728. Accuracy: 76.5\n",
      "Iteration: 94. Loss: 1.2849786281585693. Accuracy: 76.78\n",
      "Iteration: 95. Loss: 1.2734814882278442. Accuracy: 77.0\n",
      "Iteration: 96. Loss: 1.2621421813964844. Accuracy: 77.16\n",
      "Iteration: 97. Loss: 1.2509719133377075. Accuracy: 77.29\n",
      "Iteration: 98. Loss: 1.2399502992630005. Accuracy: 77.48\n",
      "Iteration: 99. Loss: 1.2291089296340942. Accuracy: 77.69\n",
      "Iteration: 100. Loss: 1.2184127569198608. Accuracy: 77.86\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "accuracy_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                images = images.view(-1, 28 * 28).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Making use of the GPU\n",
    "                '''\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "                '''\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            accuracy_history.append(accuracy)\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a  plot of Accuracy vs Iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "s = interpolate.InterpolatedUnivariateSpline(range(len(accuracy_history)), accuracy_history)\n",
    "xnew = np.arange(0, 100, 1)\n",
    "ynew=s(xnew)\n",
    "#plt.figure()\n",
    "plt.plot(xnew, ynew)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accuracy vs Iteration')\n",
    "plt.show()\n",
    "#plt.plot(accuracy_history)\n",
    "#plt.xlabel(\"Iteration\")\n",
    "#plt.ylabel(\"Accuracy\")\n",
    "#plt.title(\"Accuracy vs Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy is even worse compared to a single-layer perceptron. We will use a different method by splitting the dataset into batches, and iterating through the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Loss: 1.8277956247329712. Accuracy: 67.3\n",
      "Iteration: 200. Loss: 1.098745584487915. Accuracy: 78.81\n",
      "Iteration: 300. Loss: 0.7527970671653748. Accuracy: 82.23\n",
      "Iteration: 400. Loss: 0.6607846617698669. Accuracy: 85.55\n",
      "Iteration: 500. Loss: 0.5529386401176453. Accuracy: 86.92\n",
      "Iteration: 600. Loss: 0.48485657572746277. Accuracy: 87.86\n",
      "Iteration: 700. Loss: 0.44814446568489075. Accuracy: 88.65\n",
      "Iteration: 800. Loss: 0.4186347424983978. Accuracy: 88.99\n",
      "Iteration: 900. Loss: 0.37985357642173767. Accuracy: 89.17\n",
      "Iteration: 1000. Loss: 0.34570449590682983. Accuracy: 89.53\n",
      "Iteration: 1100. Loss: 0.4103255271911621. Accuracy: 89.72\n",
      "Iteration: 1200. Loss: 0.3532769978046417. Accuracy: 89.93\n",
      "Iteration: 1300. Loss: 0.34923815727233887. Accuracy: 90.1\n",
      "Iteration: 1400. Loss: 0.36477532982826233. Accuracy: 90.27\n",
      "Iteration: 1500. Loss: 0.36095118522644043. Accuracy: 90.45\n",
      "Iteration: 1600. Loss: 0.34261104464530945. Accuracy: 90.49\n",
      "Iteration: 1700. Loss: 0.35779428482055664. Accuracy: 90.71\n",
      "Iteration: 1800. Loss: 0.3098052442073822. Accuracy: 90.77\n",
      "Iteration: 1900. Loss: 0.2807968258857727. Accuracy: 90.95\n",
      "Iteration: 2000. Loss: 0.2997817099094391. Accuracy: 91.02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "batch_size = 600\n",
    "n_iters = 2000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "input_dim = 28*28\n",
    "hidden_dim = 500\n",
    "output_dim = 10\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "accuracy_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                images = images.view(-1, 28 * 28).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                '''\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "                '''\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            accuracy_history.append(accuracy)\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 2.2198796129226683 TrainAcc: 23.246666595339775 Test loss: 2.0964239485123577 Test Acc: 42.92156836565803\n",
      "Epoch: 2 Train loss: 1.9705441558361054 TrainAcc: 47.33666676282883 Test loss: 1.7757966588525211 Test Acc: 68.19117595167721\n",
      "Epoch: 3 Train loss: 1.626845668554306 TrainAcc: 61.07499951124191 Test loss: 1.4014984018662398 Test Acc: 73.14705918816959\n",
      "Epoch: 4 Train loss: 1.2993903398513793 TrainAcc: 69.32833361625671 Test loss: 1.1073306974242716 Test Acc: 79.11764768993153\n",
      "Epoch: 5 Train loss: 1.0606046241521836 TrainAcc: 74.2216666340828 Test loss: 0.9097881527508006 Test Acc: 81.80392174159779\n",
      "Epoch: 6 Train loss: 0.902889894247055 TrainAcc: 77.3850000500679 Test loss: 0.7803307526251849 Test Acc: 83.24999984572915\n",
      "Epoch: 7 Train loss: 0.796648052930832 TrainAcc: 79.41166687011719 Test loss: 0.6914655250661513 Test Acc: 84.29901985561146\n",
      "Epoch: 8 Train loss: 0.7218539905548096 TrainAcc: 80.68333369493484 Test loss: 0.6268200365936055 Test Acc: 85.44607793583589\n",
      "Epoch: 9 Train loss: 0.6675220721960068 TrainAcc: 81.87999987602234 Test loss: 0.5793597330065334 Test Acc: 85.9460778096143\n",
      "Epoch: 10 Train loss: 0.625824893116951 TrainAcc: 82.81499916315079 Test loss: 0.542955615941216 Test Acc: 86.92647078458\n",
      "Epoch: 11 Train loss: 0.5913661593198776 TrainAcc: 83.59833300113678 Test loss: 0.5133979373118457 Test Acc: 87.30392175562241\n",
      "Epoch: 12 Train loss: 0.565253104865551 TrainAcc: 84.10333293676376 Test loss: 0.48899363419588876 Test Acc: 87.75000011219697\n",
      "Epoch: 13 Train loss: 0.5423780632019043 TrainAcc: 84.64833354949951 Test loss: 0.4703034670913921 Test Acc: 87.81862749772914\n",
      "Epoch: 14 Train loss: 0.5236069458723068 TrainAcc: 85.1250005364418 Test loss: 0.4524042334626703 Test Acc: 88.28921563485089\n",
      "Epoch: 15 Train loss: 0.5090041619539261 TrainAcc: 85.45000046491623 Test loss: 0.4389534434851478 Test Acc: 88.4607847999124\n",
      "Epoch: 16 Train loss: 0.49337239891290663 TrainAcc: 85.84166729450226 Test loss: 0.4267343282699585 Test Acc: 88.75490216647877\n",
      "Epoch: 17 Train loss: 0.48409471303224566 TrainAcc: 86.12500077486038 Test loss: 0.4159599796814077 Test Acc: 88.9166674193214\n",
      "Epoch: 18 Train loss: 0.471942675113678 TrainAcc: 86.3900004029274 Test loss: 0.4069583854254554 Test Acc: 89.16176557540894\n",
      "Epoch: 19 Train loss: 0.46370875030756 TrainAcc: 86.37833386659622 Test loss: 0.39944834481267366 Test Acc: 89.11274601431454\n",
      "Epoch: 20 Train loss: 0.4554868277907371 TrainAcc: 86.72000062465668 Test loss: 0.3919932517935248 Test Acc: 89.25000148660996\n",
      "Epoch: 21 Train loss: 0.44712974518537524 TrainAcc: 87.07833367586136 Test loss: 0.3851320059860454 Test Acc: 89.55392276539521\n",
      "Epoch: 22 Train loss: 0.4388789224624634 TrainAcc: 87.20333355665207 Test loss: 0.378829126848894 Test Acc: 89.42156995044036\n",
      "Epoch: 23 Train loss: 0.43439447939395903 TrainAcc: 87.3850000500679 Test loss: 0.37396909427993436 Test Acc: 89.52941298484802\n",
      "Epoch: 24 Train loss: 0.42673185348510745 TrainAcc: 87.52499997615814 Test loss: 0.3692992724039975 Test Acc: 89.60294197587406\n",
      "Epoch: 25 Train loss: 0.4226763117313385 TrainAcc: 87.60166662931442 Test loss: 0.36542142270242467 Test Acc: 89.76470687810112\n",
      "Epoch: 26 Train loss: 0.4181019467115402 TrainAcc: 87.74333328008652 Test loss: 0.3607078663566533 Test Acc: 89.73039318533505\n",
      "Epoch: 27 Train loss: 0.4145872658491135 TrainAcc: 87.90499967336655 Test loss: 0.3566146326415679 Test Acc: 89.87745186861824\n",
      "Epoch: 28 Train loss: 0.41173249423503877 TrainAcc: 87.91333293914795 Test loss: 0.3536298993755789 Test Acc: 89.93137338582207\n",
      "Epoch: 29 Train loss: 0.40419724345207214 TrainAcc: 88.20666605234146 Test loss: 0.35048527621171055 Test Acc: 90.10294184965245\n",
      "Epoch: 30 Train loss: 0.40331121504306794 TrainAcc: 88.09666633605957 Test loss: 0.34691766211215186 Test Acc: 90.0588242446675\n",
      "Epoch: 31 Train loss: 0.39845406532287597 TrainAcc: 88.3749994635582 Test loss: 0.34450804003897834 Test Acc: 90.00000077135422\n",
      "Epoch: 32 Train loss: 0.3959371080994606 TrainAcc: 88.48333269357681 Test loss: 0.3413918859818402 Test Acc: 90.3186279184678\n",
      "Epoch: 33 Train loss: 0.39221728563308716 TrainAcc: 88.46333247423172 Test loss: 0.3389214391217512 Test Acc: 90.20098097184125\n",
      "Epoch: 34 Train loss: 0.38719018012285233 TrainAcc: 88.5249993801117 Test loss: 0.3370025188607328 Test Acc: 90.26470640126396\n",
      "Epoch: 35 Train loss: 0.38624447226524355 TrainAcc: 88.47999942302704 Test loss: 0.3347187414765358 Test Acc: 90.26470640126396\n",
      "Epoch: 36 Train loss: 0.3851424998044968 TrainAcc: 88.70499908924103 Test loss: 0.33279035853988986 Test Acc: 90.4411768212038\n",
      "Epoch: 37 Train loss: 0.38088139504194257 TrainAcc: 88.82666563987732 Test loss: 0.33065905465799217 Test Acc: 90.37745139178108\n",
      "Epoch: 38 Train loss: 0.3778272974491119 TrainAcc: 88.90999913215637 Test loss: 0.3285829687819761 Test Acc: 90.6029413728153\n",
      "Epoch: 39 Train loss: 0.3748961016535759 TrainAcc: 89.00499898195267 Test loss: 0.32643528326469307 Test Acc: 90.6029413728153\n",
      "Epoch: 40 Train loss: 0.37609961807727815 TrainAcc: 88.89166593551636 Test loss: 0.3250870441689211 Test Acc: 90.68137267056633\n",
      "Epoch: 41 Train loss: 0.3732100409269333 TrainAcc: 89.01999890804291 Test loss: 0.3237144105574664 Test Acc: 90.69607853889465\n",
      "Epoch: 42 Train loss: 0.3712623772025108 TrainAcc: 88.98166579008102 Test loss: 0.32185833550551357 Test Acc: 90.66176484612858\n",
      "Epoch: 43 Train loss: 0.3689798215031624 TrainAcc: 89.05833232402802 Test loss: 0.3203260434024474 Test Acc: 90.82843135384952\n",
      "Epoch: 44 Train loss: 0.3642747077345848 TrainAcc: 89.24833261966705 Test loss: 0.31923967117772384 Test Acc: 90.84803917828728\n",
      "Epoch: 45 Train loss: 0.3646763974428177 TrainAcc: 89.1999990940094 Test loss: 0.3176957504714237 Test Acc: 90.7647059244268\n",
      "Epoch: 46 Train loss: 0.3629221004247665 TrainAcc: 89.36166566610336 Test loss: 0.3166447649983799 Test Acc: 90.79901961719288\n",
      "Epoch: 47 Train loss: 0.36175456702709197 TrainAcc: 89.27166575193405 Test loss: 0.3146490073379348 Test Acc: 90.93137243214775\n",
      "Epoch: 48 Train loss: 0.3594716614484787 TrainAcc: 89.51166611909866 Test loss: 0.3139563926002559 Test Acc: 90.82352939774009\n",
      "Epoch: 49 Train loss: 0.3581755357980728 TrainAcc: 89.4566661119461 Test loss: 0.31245311042841745 Test Acc: 90.99019590546104\n",
      "Epoch: 50 Train loss: 0.35529788345098495 TrainAcc: 89.50499933958054 Test loss: 0.3116622584707597 Test Acc: 90.9999998176799\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "batch_size = 600\n",
    "n_iters = 2000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "input_dim = 28*28\n",
    "hidden_dim = 512\n",
    "output_dim = 10\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_dim = 28*28\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.sigmoid(self.fc1(x)))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = FeedforwardNeuralNetModel()\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "min_loss = np.Inf\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0 \n",
    "    test_loss = 0 \n",
    "    test_acc = 0 \n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model.forward(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(top_class.shape)\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "    # Validation Step\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for images, labels in testloader:\n",
    "            log_ps = model.forward(images)\n",
    "            test_loss += criterion(log_ps, labels).item()\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(top_class.shape)\n",
    "            test_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "    \n",
    "    train_losses.append(train_loss/len(trainloader))\n",
    "    test_losses.append(test_loss/len(testloader))\n",
    "    \n",
    "    print(\"Epoch:\",i+1,\n",
    "          \"Train loss:\",train_loss/len(trainloader),\n",
    "          \"TrainAcc:\",100*train_acc/len(trainloader),\n",
    "          \"Test loss:\",test_loss/len(testloader),\n",
    "          \"Test Acc:\",100*test_acc/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#model.fc2.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint = {'model': FeedforwardNeuralNetModel(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, './asset/model/FNN1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn our attention to the case of a FNN with 3 hidden layers. The gross format of the code is quite similar to the one with 1 hidden layer, except in the model class, the 2 extra layers are stated. Note, the same batch processing method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Loss: 2.3026442527770996. Accuracy: 11.35\n",
      "Iteration: 200. Loss: 2.302074909210205. Accuracy: 10.28\n",
      "Iteration: 300. Loss: 2.306206226348877. Accuracy: 11.35\n",
      "Iteration: 400. Loss: 2.302682638168335. Accuracy: 11.35\n",
      "Iteration: 500. Loss: 2.3025224208831787. Accuracy: 10.32\n",
      "Iteration: 600. Loss: 2.305614471435547. Accuracy: 11.35\n",
      "Iteration: 700. Loss: 2.29923939704895. Accuracy: 10.28\n",
      "Iteration: 800. Loss: 2.3016533851623535. Accuracy: 10.1\n",
      "Iteration: 900. Loss: 2.300278902053833. Accuracy: 11.35\n",
      "Iteration: 1000. Loss: 2.296316623687744. Accuracy: 11.35\n",
      "Iteration: 1100. Loss: 2.2997264862060547. Accuracy: 11.35\n",
      "Iteration: 1200. Loss: 2.3005809783935547. Accuracy: 11.35\n",
      "Iteration: 1300. Loss: 2.297628879547119. Accuracy: 11.35\n",
      "Iteration: 1400. Loss: 2.305021286010742. Accuracy: 9.8\n",
      "Iteration: 1500. Loss: 2.299114465713501. Accuracy: 11.35\n",
      "Iteration: 1600. Loss: 2.3034605979919434. Accuracy: 11.35\n",
      "Iteration: 1700. Loss: 2.2945854663848877. Accuracy: 10.28\n",
      "Iteration: 1800. Loss: 2.2962400913238525. Accuracy: 11.35\n",
      "Iteration: 1900. Loss: 2.296353340148926. Accuracy: 19.57\n",
      "Iteration: 2000. Loss: 2.297018527984619. Accuracy: 11.35\n",
      "Iteration: 2100. Loss: 2.298457622528076. Accuracy: 11.35\n",
      "Iteration: 2200. Loss: 2.293320655822754. Accuracy: 11.35\n",
      "Iteration: 2300. Loss: 2.295011520385742. Accuracy: 11.35\n",
      "Iteration: 2400. Loss: 2.291618585586548. Accuracy: 10.28\n",
      "Iteration: 2500. Loss: 2.2893526554107666. Accuracy: 10.09\n",
      "Iteration: 2600. Loss: 2.280102014541626. Accuracy: 26.88\n",
      "Iteration: 2700. Loss: 2.2647545337677. Accuracy: 28.74\n",
      "Iteration: 2800. Loss: 2.248633861541748. Accuracy: 27.93\n",
      "Iteration: 2900. Loss: 2.208298921585083. Accuracy: 29.53\n",
      "Iteration: 3000. Loss: 2.1182868480682373. Accuracy: 32.41\n",
      "Iteration: 3100. Loss: 1.9296388626098633. Accuracy: 35.58\n",
      "Iteration: 3200. Loss: 1.6984105110168457. Accuracy: 39.22\n",
      "Iteration: 3300. Loss: 1.5124937295913696. Accuracy: 43.3\n",
      "Iteration: 3400. Loss: 1.4111945629119873. Accuracy: 46.73\n",
      "Iteration: 3500. Loss: 1.318497896194458. Accuracy: 49.3\n",
      "Iteration: 3600. Loss: 1.2651323080062866. Accuracy: 53.21\n",
      "Iteration: 3700. Loss: 1.223942518234253. Accuracy: 54.88\n",
      "Iteration: 3800. Loss: 1.1939857006072998. Accuracy: 58.01\n",
      "Iteration: 3900. Loss: 1.1265829801559448. Accuracy: 59.97\n",
      "Iteration: 4000. Loss: 1.1196495294570923. Accuracy: 62.44\n",
      "Iteration: 4100. Loss: 1.0529298782348633. Accuracy: 65.04\n",
      "Iteration: 4200. Loss: 1.0184011459350586. Accuracy: 66.88\n",
      "Iteration: 4300. Loss: 0.9785979986190796. Accuracy: 68.66\n",
      "Iteration: 4400. Loss: 1.013582468032837. Accuracy: 70.25\n",
      "Iteration: 4500. Loss: 0.8958318829536438. Accuracy: 71.79\n",
      "Iteration: 4600. Loss: 0.8626030683517456. Accuracy: 74.38\n",
      "Iteration: 4700. Loss: 0.8153483271598816. Accuracy: 76.02\n",
      "Iteration: 4800. Loss: 0.7764052748680115. Accuracy: 77.75\n",
      "Iteration: 4900. Loss: 0.7047262787818909. Accuracy: 79.55\n",
      "Iteration: 5000. Loss: 0.7307069897651672. Accuracy: 80.8\n",
      "Iteration: 5100. Loss: 0.7462071776390076. Accuracy: 81.73\n",
      "Iteration: 5200. Loss: 0.6493523716926575. Accuracy: 82.6\n",
      "Iteration: 5300. Loss: 0.5867075324058533. Accuracy: 83.18\n",
      "Iteration: 5400. Loss: 0.6109905242919922. Accuracy: 83.56\n",
      "Iteration: 5500. Loss: 0.5615388751029968. Accuracy: 84.1\n",
      "Iteration: 5600. Loss: 0.5583655834197998. Accuracy: 84.5\n",
      "Iteration: 5700. Loss: 0.5729137659072876. Accuracy: 84.97\n",
      "Iteration: 5800. Loss: 0.5107548236846924. Accuracy: 85.57\n",
      "Iteration: 5900. Loss: 0.5784612894058228. Accuracy: 85.97\n",
      "Iteration: 6000. Loss: 0.49953439831733704. Accuracy: 86.18\n",
      "Iteration: 6100. Loss: 0.5022062063217163. Accuracy: 86.56\n",
      "Iteration: 6200. Loss: 0.6171226501464844. Accuracy: 86.86\n",
      "Iteration: 6300. Loss: 0.43475332856178284. Accuracy: 87.15\n",
      "Iteration: 6400. Loss: 0.4549598693847656. Accuracy: 87.4\n",
      "Iteration: 6500. Loss: 0.462677925825119. Accuracy: 87.66\n",
      "Iteration: 6600. Loss: 0.48795902729034424. Accuracy: 87.95\n",
      "Iteration: 6700. Loss: 0.486907958984375. Accuracy: 88.0\n",
      "Iteration: 6800. Loss: 0.4690982699394226. Accuracy: 88.25\n",
      "Iteration: 6900. Loss: 0.3775462210178375. Accuracy: 88.53\n",
      "Iteration: 7000. Loss: 0.5164343118667603. Accuracy: 88.72\n",
      "Iteration: 7100. Loss: 0.4172723591327667. Accuracy: 88.93\n",
      "Iteration: 7200. Loss: 0.40865591168403625. Accuracy: 89.19\n",
      "Iteration: 7300. Loss: 0.387587308883667. Accuracy: 89.22\n",
      "Iteration: 7400. Loss: 0.43888604640960693. Accuracy: 89.42\n",
      "Iteration: 7500. Loss: 0.4070185124874115. Accuracy: 89.49\n",
      "Iteration: 7600. Loss: 0.3760959208011627. Accuracy: 89.59\n",
      "Iteration: 7700. Loss: 0.37236955761909485. Accuracy: 89.81\n",
      "Iteration: 7800. Loss: 0.3568122386932373. Accuracy: 89.79\n",
      "Iteration: 7900. Loss: 0.4170796573162079. Accuracy: 90.06\n",
      "Iteration: 8000. Loss: 0.4344545006752014. Accuracy: 90.14\n",
      "Iteration: 8100. Loss: 0.3373834490776062. Accuracy: 90.25\n",
      "Iteration: 8200. Loss: 0.3656569719314575. Accuracy: 90.4\n",
      "Iteration: 8300. Loss: 0.38034960627555847. Accuracy: 90.4\n",
      "Iteration: 8400. Loss: 0.30244553089141846. Accuracy: 90.56\n",
      "Iteration: 8500. Loss: 0.39016273617744446. Accuracy: 90.66\n",
      "Iteration: 8600. Loss: 0.37929320335388184. Accuracy: 90.82\n",
      "Iteration: 8700. Loss: 0.3592112362384796. Accuracy: 90.83\n",
      "Iteration: 8800. Loss: 0.31860190629959106. Accuracy: 91.05\n",
      "Iteration: 8900. Loss: 0.3210059404373169. Accuracy: 91.15\n",
      "Iteration: 9000. Loss: 0.2912556529045105. Accuracy: 91.21\n",
      "Iteration: 9100. Loss: 0.3076431155204773. Accuracy: 91.19\n",
      "Iteration: 9200. Loss: 0.3551841676235199. Accuracy: 91.37\n",
      "Iteration: 9300. Loss: 0.4015444815158844. Accuracy: 91.54\n",
      "Iteration: 9400. Loss: 0.3026319742202759. Accuracy: 91.62\n",
      "Iteration: 9500. Loss: 0.365967333316803. Accuracy: 91.59\n",
      "Iteration: 9600. Loss: 0.35264870524406433. Accuracy: 91.61\n",
      "Iteration: 9700. Loss: 0.3029348850250244. Accuracy: 91.74\n",
      "Iteration: 9800. Loss: 0.282723069190979. Accuracy: 91.83\n",
      "Iteration: 9900. Loss: 0.3070806860923767. Accuracy: 91.88\n",
      "Iteration: 10000. Loss: 0.2894343137741089. Accuracy: 92.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 600\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.Sigmoid()\n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.Sigmoid()\n",
    "        \n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "accuracy_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                images = images.view(-1, 28 * 28).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            accuracy_history.append(accuracy)\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "s = interpolate.InterpolatedUnivariateSpline(range(len(accuracy_history)), accuracy_history)\n",
    "xnew = np.arange(0, 100, 1)\n",
    "ynew=s(xnew)\n",
    "\n",
    "plt.plot(100*xnew, ynew)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accuracy vs Iteration')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
